Metadata-Version: 2.4
Name: leadsearch
Version: 0.1.0
Summary: Hybrid (lexical + vector) multi-dataset search over large lead datasets (8M + 100k) with progress reporting.
Author: AutoGenerated
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: pandas>=2.2.0
Requires-Dist: tqdm>=4.66.0
Requires-Dist: sentence-transformers>=2.7.0
Requires-Dist: faiss-cpu>=1.7.4; platform_system != "Windows"
Requires-Dist: fastapi>=0.111.0
Requires-Dist: uvicorn>=0.30.0
Requires-Dist: openpyxl>=3.1.2
Requires-Dist: sqlite-utils>=3.36
Requires-Dist: numpy>=1.26.0
Requires-Dist: orjson>=3.10.0
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: hnswlib>=0.8.0
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: ruff; extra == "dev"

# Lead Search - Hybrid Dataset Search Engine

A high-performance hybrid search system for exploring 8M+ lead datasets with lexical (SQLite FTS5) + vector (FAISS) search capabilities and an interactive Streamlit web interface.

## 🚀 Quick Start

### Installation

```bash
# Clone/navigate to project
cd leadsearching

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install project and dependencies
pip install -e .

# Optional: Install Streamlit for web UI
pip install streamlit plotly
```

### Dataset Structure
This project works with two datasets:
- **8 Million Leads**: 17 CSV files (~100-115MB each) + 1 video
- **100k Leads**: 1 XLSX file (~11MB)

**Schema**: `username,name,bio,category,followerCount,followingCount,website,email,phone`

## 🎯 Core Features

### 1. **Hybrid Search Architecture**
- **Lexical Search**: SQLite FTS5 for exact term matching
- **Vector Search**: FAISS with sentence transformers for semantic similarity
- **Score Fusion**: Reciprocal rank fusion for optimal result ranking

### 2. **Streamlit Web Interface**
```bash
streamlit run streamlit_app.py --server.port 8502
```

**Features:**
- 📊 **Dataset Overview**: Statistics, record counts, category breakdowns
- 📥 **Data Ingestion**: Progress monitoring, batch processing, file pattern matching
- 🔍 **Interactive Search**: Hybrid/lexical/vector modes, filtering, export
- 📈 **Results Analytics**: Category distribution, follower analysis, relevance scoring

### 3. **Command Line Interface**

#### Ingest Data
```bash
python -c "from leadsearch.cli import main; main()" ingest \
  --zip "8 MILLION LEADS-20250921T154416Z-1-001.zip" \
  --pattern "*.csv" \
  --dataset "8million" \
  --limit 5000  # Optional: for testing

python -c "from leadsearch.cli import main; main()" ingest \
  --zip "100k Sales Link 7 file-20250920T133656Z-1-001.zip" \
  --pattern "*.xlsx" \
  --dataset "100k" \
  --no-vectors  # Optional: skip embeddings for faster ingestion
```

#### Search Data
```bash
# Hybrid search (default)
python -c "from leadsearch.cli import main; main()" search \
  --query "startup founder tech email" \
  --limit 50

# Lexical only
python -c "from leadsearch.cli import main; main()" search \
  --query "marketing agency" \
  --mode lexical \
  --limit 100

# Vector/semantic only
python -c "from leadsearch.cli import main; main()" search \
  --query "business development" \
  --mode vector \
  --limit 25
```

#### Monitor Progress
```bash
python -c "from leadsearch.cli import main; main()" status
```

## 🏗️ Architecture

### **Data Pipeline**
```
ZIP Archives → Streaming Extraction → CSV/XLSX Parsing → 
Batch Processing (5k chunks) → SQLite + FTS5 → Vector Embeddings → FAISS Index
```

### **Search Pipeline**
```
Query → Lexical Search (SQLite FTS5) + Vector Search (FAISS) → 
Score Fusion → Ranking → Results
```

### **Project Structure**
```
leadsearching/
├── src/leadsearch/
│   ├── config.py          # Settings management
│   ├── progress.py        # JSON-based progress tracking
│   ├── db.py             # SQLite schema & operations
│   ├── embedding.py      # Sentence transformer model
│   ├── vector_index.py   # FAISS/HNSW abstraction
│   ├── ingest.py         # Streaming ingestion pipeline
│   ├── search.py         # Hybrid search implementation
│   ├── api.py            # FastAPI REST endpoints
│   └── cli.py            # Command-line interface
├── streamlit_app.py      # Web interface
├── tests/                # Unit tests
├── data/                 # Generated data directory
│   ├── leads.db         # SQLite database
│   ├── index/           # Vector index files
│   └── status.json      # Ingestion progress
└── pyproject.toml       # Project configuration
```

## ⚙️ Configuration

Environment variables (optional):
```bash
export LEADSEARCH_DB_PATH="./data/leads.db"
export LEADSEARCH_INDEX_DIR="./data/index"
export LEADSEARCH_BATCH_SIZE=5000
export LEADSEARCH_MODEL_NAME="all-MiniLM-L6-v2"
export LEADSEARCH_FLUSH_EVERY=10
```

## 🔧 Technical Details

### **Dependencies**
- **Data Processing**: pandas, tqdm, openpyxl
- **Search**: sqlite-utils (FTS5), faiss-cpu, sentence-transformers
- **API**: FastAPI, uvicorn
- **UI**: Streamlit, plotly
- **Storage**: SQLite, FAISS, numpy

### **Performance Optimizations**
- **Streaming Processing**: Handle large ZIP files without memory issues
- **Batch Ingestion**: 5k record chunks for optimal throughput
- **Progress Tracking**: JSON-based status with throttled writes
- **Vector Quantization**: fp16 storage option for memory efficiency
- **Index Persistence**: Save/load trained FAISS indexes

### **Search Modes**
- **`hybrid`** (default): Combines lexical + vector with RRF scoring
- **`lexical`**: SQLite FTS5 full-text search only
- **`vector`**: Semantic similarity via sentence transformers only

### **Quality Assurance**
- **Linting**: Ruff with modern Python type hints (PEP 585)
- **Code Style**: 100-character line limit, import sorting
- **Error Handling**: Comprehensive exception management
- **Type Safety**: Full type annotations throughout

## 📊 Usage Examples

### **Web Interface Workflow**
1. Launch: `streamlit run streamlit_app.py --server.port 8502`
2. Navigate to **Dataset Overview** for current statistics
3. Use **Ingestion** tab to process ZIP files with progress monitoring
4. **Search** tab for interactive queries with filtering and analytics
5. Export results as CSV for further analysis

### **Programmatic Usage**
```python
from leadsearch.config import get_settings
from leadsearch.db import connect
from leadsearch.search import hybrid_search

settings = get_settings()
conn = connect(settings.db_path)

results = hybrid_search(
    conn=conn,
    query="startup founder email",
    mode="hybrid",
    limit=100,
    min_score=0.2,
    filters={"category": "tech", "follower_min": 1000}
)

print(f"Found {len(results)} results")
for result in results[:5]:
    print(f"• {result['name']} ({result['username']}) - Score: {result['score']:.3f}")
```

## 🚀 Production Deployment

### **FastAPI REST Service**
```bash
python -c "from leadsearch.cli import main; main()" api --host 0.0.0.0 --port 8000
```

### **Docker Deployment** (Future)
```dockerfile
FROM python:3.11-slim
COPY . /app
WORKDIR /app
RUN pip install -e .
EXPOSE 8000
CMD ["python", "-c", "from leadsearch.cli import main; main()", "api"]
```

## 📈 Performance Metrics

### **Expected Throughput**
- **Ingestion**: ~500-1000 records/second (with vectors)
- **Search**: <100ms response time for hybrid queries
- **Storage**: ~2GB for 8M records (including vectors)

### **Memory Usage**
- **Ingestion**: ~200-500MB peak (streaming processing)
- **Search**: ~1-2GB (loaded embeddings + models)
- **Vectors**: ~800MB (all-MiniLM-L6-v2 embeddings for 8M records)

## 🤝 Contributing

1. **Code Style**: Follows Ruff linting rules with modern Python patterns
2. **Type Safety**: Full type annotations required
3. **Testing**: Add unit tests for new functionality
4. **Documentation**: Update README and docstrings

## 📄 License

This project is developed for lead dataset exploration and search capabilities.

---

**🔍 Ready to explore 8M+ leads with hybrid AI-powered search!**

*Built with Python, SQLite FTS5, FAISS, and Streamlit*
